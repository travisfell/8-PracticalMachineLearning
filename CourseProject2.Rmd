---
title: "Classe Prediction"
author: "Travis Fell"
date: "September 24, 2015"
output: html_document
---
#I. Introduction
 

#II. Set Up Work
First, we'll start out by loading libraries, loading data and doing a bit of exploratory analysis. 
```{r}
setwd("C:/Users/fellt/Desktop/Data Science/Coursera Data Science Specialization/08 - Practical Machine Learning/8-PracticalMachineLearning")
set.seed(333)
library(caret)
library(randomForest)
library(ggplot2)
library(plyr)
library(dplyr)
library(GGally)
library(parallel, quietly=T)
library(doParallel, quietly=T)
train <- read.csv("pml-training.csv", stringsAsFactors = FALSE, na.strings=c("", "NA"))
test <- read.csv("pml-testing.csv", stringsAsFactors = FALSE, na.strings=c("", "NA"))
train$classe <- as.factor(train$classe)
#str(train)
#summary(train)
#table(train$classe)


#reducing the data set 

#remove first 7 columns from both data sets as they are only informational about the subjects and test
train <- train[,8:160]
test <- test[,8:160]

# filter out columns with majority NA values
train <- train[sapply(train, function(x) !any(is.na(x)))] 
test <- test[sapply(test, function(x) !any(is.na(x)))]

# might use nearZeroVar() here instead

# find columns with more than 15% of rows = 0 and remove
  zerotest <<- NULL
  for(i in 1:length(train))
    {
    zerotest <<- c(zerotest, nrow(train[train[,i] == 0,])/nrow(train))
    #train[,i] <- suppressWarnings(as.numeric(train[,i]))
  }
removecols <- colnames(train[,which(zerotest >.15)])
train[,removecols] <- list(NULL)
test[,removecols] <- list(NULL)

# find records with any 0 values and remove
train[train == 0] <- NA
train <- train[complete.cases(train),]

# create new classe column to store numeric values equivalent to the original classe varible
train$classe2 <- NA
train$classe2[train$classe == 'A'] <- 1
train$classe2[train$classe == 'B'] <- 2
train$classe2[train$classe == 'C'] <- 3
train$classe2[train$classe == 'D'] <- 4
train$classe2[train$classe == 'E'] <- 5
train$classe <- train$classe2
train$classe2 <- NULL
train$classe <- as.integer(train$classe)

#find correlations in remaining features to ID add'l variables to exclude
traincor <- findCorrelation(cor(train), cutof = .8, verbose = FALSE)
train <- train[,-traincor] #exclude 1 feature from each pair of highly correlated features from training set
test <- test[, -traincor] #exclude same features from test set


#exploring predictors 
qplot(total_accel_arm, total_accel_dumbbell, color = classe , data = train)
featurePlot(x = train[,c("total_accel_belt", "total_accel_arm", "total_accel_forearm", "total_accel_dumbbell")], y = train$classe, plot = "pairs")
featurePlot(x = train[,c(1,5,9,13)], y = train$classe, plot = "pairs")
qplot(total_accel_arm, classe, data = train)


#selecting predictors
#use all columns with "accel" in name per the assignment your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell 

trainaccel <- select(train, contains("accel"))
trainaccel <- cbind(trainaccel, train$classe)
colnames(trainaccel)[length(trainaccel)] <- "classe"

#prepare test set for accel columns
testaccelcols <- colnames(select(test, contains("accel")))
testaccel <- subset(test, select = c(testaccelcols, "problem_id"))

#set up a multiple linear regression on acceleration features
mrmAccel <- train(classe ~ ., method = "lm", data = trainaccel)
print(mrmAccel)
finmod <- mrmAccel$finalModel
print(finmod)

#START HERE

#use the model to predict the validation set and estimate out of sample error

# split the training set into 70/30 training-validation sets
inTrain <- createDataPartition(trainaccel$classe, p = .7, list = FALSE)
train2 <- train[inTrain,]
validation <- train[-inTrain,]

validationPred <- predict(mrmAccel, validation)
summary(validationPred)
table(validationPred, validation$classe)


#make predictions
pred_mrmAccel <- predict(mrmAccel, test)
summary(pred_mrmAccel)

```
Which method to use? 
Apply parallel processing techniques (have 4 cores, use 2)
Note: use SaveRDS to save models once created. 


Now that we've narrowed down the number of potential predictors to a more manageable number, let's have a quick look at how this data looks

To predict X, use data related to X. 

## Cross Validation
Cut the training set into a training and validation set. 


## Building the Model
Note: Follow the longer process from the slides to break down the process into component parts and debug. 

## Estimating Out of Sample Error
Using the model fit above, we will estimate the out of sample error using the validation set. 

## Making Predictions
We will now run the model against the test set to make predictions about the classe variable as well as identify the out of sample error to that performed on the validation set above. 






This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


#Appendix: Old Code
Putting old code here in case I need it again.

```{r}

# only use accel columns as predictors
train2 <- select(train, contains("accel"))
train2 <- cbind(train2, train$classe)
colnames(train2)[17] <- "classe"
train <- train2
rm(train2)

#prepare test set for accel columns
select(test, contains("accel")))
testaccelcols <- colnames(select(test, contains("accel")))
test <- subset(test, select = c(testaccelcols, "problem_id"))

```

