---
title: "But How Well Are You Doing It? Predicting from Fitbit Data"
author: "Travis Fell"
date: "November 15, 2015"
output: html_document
---
#I. Introduction
This analysis predicts how well test subjects would perform a dumbbell bicep curl from personal movement tracking devices like Nike FuelBand and Fitbits. 

Using a training set of measurements from these personal exercise movement devices like Nike FuelBands and FitBits as well as assessments from professional trainers on the correctness of exercises performed, this analysis will predict how well 20 test subjects performed the same exercise. Exercise performance is categorized by the classe column in the training set using the following levels:
 - A: Exactly according to the specification 
 - B: Throwing the elbows to the front 
 - C: Lifting the dumbbell only halfway
 - D: Lowering the dumbbell only halfway
 - E: Throwing the hips to the front
Read more here: http://groupware.les.inf.puc-rio.br/har 

#II. Set Up Work
First, we'll start out by loading libraries, loading data and doing a bit of exploratory analysis. 
```{r}
setwd("C:/Users/fellt/Desktop/Data Science/Coursera Data Science Specialization/08 - Practical Machine Learning/8-PracticalMachineLearning")
set.seed(333)
library(caret)
library(randomForest)
library(ggplot2)
library(dplyr)
library(parallel, quietly=T)
library(doParallel, quietly=T)

train <- read.csv("pml-training.csv", stringsAsFactors = FALSE, na.strings=c("", " ", "NA"))
test <- read.csv("pml-testing.csv", stringsAsFactors = FALSE, na.strings=c("", " ", "NA"))


#reducing the data set 

#remove first 7 columns from both data sets as they are only informational about the subjects and test
train <- train[,8:160]
test <- test[,8:160]

# filter out columns with majority NA values
train <- train[sapply(train, function(x) !any(is.na(x)))] 
test <- test[sapply(test, function(x) !any(is.na(x)))]

# might use nearZeroVar() here instead

# find columns with more than 15% of rows = 0 and remove
  zerotest <<- NULL
  for(i in 1:length(train))
    {
    zerotest <<- c(zerotest, nrow(train[train[,i] == 0,])/nrow(train))
  }
removecols <- colnames(train[,which(zerotest >.15)])
train[,removecols] <- list(NULL)
test[,removecols] <- list(NULL)


#find correlations in remaining features to ID add'l variables to exclude
traincor <- findCorrelation(cor(train[,1:45]), cutof = .8, verbose = FALSE) #leave off dependent variable for this command
train <- train[,-traincor] #exclude 1 feature from each pair of highly correlated features from training set
test <- test[, -traincor] #exclude same features from test set

# find records with any 0 values and remove
train[train == 0] <- NA
train <- train[complete.cases(train),]
train$classe <- as.factor(train$classe)


#exploring predictors 
qplot(total_accel_arm, total_accel_dumbbell, color = classe , data = train)
featurePlot(x = train[,c("total_accel_belt", "total_accel_arm", "total_accel_forearm", "total_accel_dumbbell")], y = train$classe, plot = "pairs")
featurePlot(x = train[,c(1,5,9,13)], y = train$classe, plot = "pairs")
qplot(total_accel_arm, classe, data = train)


#selecting predictors

#need to split the training set into training and validations sets here to estimate OOS error


# split the training set into 70/30 training-validation sets
inTrain <- createDataPartition(train$classe, p = .7, list = FALSE)
trainMod <- train[inTrain,]
validMod <- train[-inTrain,]

#Try out different models

# turn on parallel processing to help improve performance
cluster <- makeCluster(detectCores() - 1)
registerDoParallel(cluster)

#create boosting model
trainGBM <- train(classe ~ ., method = "gbm", data = trainMod, verbose = FALSE)
save(trainGBM, file = "boostingMod.rda")
print(trainGBM)

#perform cross validation and estimate OOS error on boosting model
validationPredGBM <- predict(trainGBM, validMod)
confusionMatrix(validMod$classe, validationPredGBM)

#create random forest model
trainRF <- randomForest(classe~., trainMod)
save(trainRF, file = "rfMod.rda")
print(trainRF)

#perform cross validation and estimate OOS error on random forest model
validationPredRF <- predict(trainRF, validMod)
confusionMatrix(validMod$classe, validationPredRF)


#make predictions on test data, save to file for submission
testPredRF <- predict(trainRF, test)

# create answers vector with values from test set prediction
testPredRF <- as.character(testPredRF)

# write to file
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}
pml_write_files(testPredRF)


```
Which method to use? 
Note: use SaveRDS to save models once created. 


Now that we've narrowed down the number of potential predictors to a more manageable number, let's have a quick look at how this data looks

To predict X, use data related to X. 

## Cross Validation
Cut the training set into a training and validation set. 


## Building the Model
Note: Follow the longer process from the slides to break down the process into component parts and debug. 

## Estimating Out of Sample Error
Using the model fit above, we will estimate the out of sample error using the validation set. 

## Making Predictions
We will now run the model against the test set to make predictions about the classe variable as well as identify the out of sample error to that performed on the validation set above. 



This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:


#Appendix: Old Code
Putting old code here in case I need it again.

```{r}
library(GGally)
library(parallel, quietly=T)
library(doParallel, quietly=T)


# create new classe column to store numeric values equivalent to the original classe varible
train$classe2 <- NA
train$classe2[train$classe == 'A'] <- 1
train$classe2[train$classe == 'B'] <- 2
train$classe2[train$classe == 'C'] <- 3
train$classe2[train$classe == 'D'] <- 4
train$classe2[train$classe == 'E'] <- 5
train$classe <- train$classe2
train$classe2 <- NULL
train$classe <- as.integer(train$classe)


# only use accel columns as predictors
train2 <- select(train, contains("accel"))
train2 <- cbind(train2, train$classe)
colnames(train2)[17] <- "classe"
train <- train2
rm(train2)

#prepare test set for accel columns
select(test, contains("accel")))
testaccelcols <- colnames(select(test, contains("accel")))
test <- subset(test, select = c(testaccelcols, "problem_id"))

#use all columns with "accel" in name per the assignment your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell 

trainaccel <- select(train, contains("accel"))
trainaccel <- cbind(trainaccel, train$classe)
colnames(trainaccel)[length(trainaccel)] <- "classe"

#prepare test set for accel columns
testaccelcols <- colnames(select(test, contains("accel")))
testaccel <- subset(test, select = c(testaccelcols, "problem_id"))

#create multiple linear regression #may pass on this in favor of RF and others
mrmAccel <- train(classe ~ ., method = "lm", data = trainaccelTrain)
print(mrmAccel)
finmod <- mrmAccel$finalModel
print(finmod)

#make predictions on test data
pred_mrmAccel <- predict(mrmAccel, test)
summary(pred_mrmAccel)
pred_mrmAccel

```

